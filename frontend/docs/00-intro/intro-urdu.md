---
id: physical-ai-intro-urdu
title: "ÙØ²ÛŒÚ©Ù„ Ø§Û’ Ø¢Ø¦ÛŒ Ø§ÙˆØ± ÛÛŒÙˆÙ…Ù†Ø§Ø¦Úˆ Ø±ÙˆØ¨ÙˆÙ¹Ú©Ø³ Ú©Ø§ ØªØ¹Ø§Ø±Ù"
sidebar_label: "ØªØ¹Ø§Ø±Ù (Ø§Ø±Ø¯Ùˆ)"
sidebar_position: 2
description: "ÙØ²ÛŒÚ©Ù„ Ø§Û’ Ø¢Ø¦ÛŒ Ú©ÛŒØ§ ÛÛ’ØŒ Ø§Ø¨ Ø±ÙˆØ¨ÙˆÙ¹Ú© Ø§Ù†Ù¹ÛŒÙ„ÛŒØ¬Ù†Ø³ Ø¨Ù†Ø§Ù†Û’ Ú©Ø§ Ø¨ÛØªØ±ÛŒÙ† ÙˆÙ‚Øª Ú©ÛŒÙˆÚº ÛÛ’ØŒ Ø§ÙˆØ± ÛŒÛ Ú©ÙˆØ±Ø³ Ø¢Ù¾ Ú©Ùˆ Ú©ÛŒØ³Û’ simulation Ø³Û’ real hardware ØªÚ© robot intelligence deploy Ú©Ø±Ù†Ø§ Ø³Ú©Ú¾Ø§Ø¦Û’ Ú¯Ø§"
keywords: [physical ai, embodied ai, urdu, Ø§Ø±Ø¯Ùˆ, humanoid robotics, ros2, isaac sim, jetson, vla models]
---

# ÙØ²ÛŒÚ©Ù„ Ø§Û’ Ø¢Ø¦ÛŒ Ø§ÙˆØ± ÛÛŒÙˆÙ…Ù†Ø§Ø¦Úˆ Ø±ÙˆØ¨ÙˆÙ¹Ú©Ø³ Ú©Ø§ ØªØ¹Ø§Ø±Ù

Ù…ØµÙ†ÙˆØ¹ÛŒ Ø°ÛØ§Ù†Øª (AI) Ú©ÛŒ Ø³Ø¨ Ø³Û’ Ø¯Ù„Ú†Ø³Ù¾ Ø³Ø±Ø­Ø¯ Ù…ÛŒÚº Ø®ÙˆØ´ Ø¢Ù…Ø¯ÛŒØ¯: **Physical AI**â€”Ø¬ÛØ§Úº Ø³Ø§ÙÙ¹ ÙˆÛŒØ¦Ø± Ú©ÛŒ Ø°ÛØ§Ù†Øª sensors, actuators, Ø§ÙˆØ± robotic hardware Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ø­Ù‚ÛŒÙ‚ÛŒ Ø¯Ù†ÛŒØ§ Ø³Û’ Ù…Ù„ØªÛŒ ÛÛ’Û”

## Physical AI Ú©ÛŒØ§ ÛÛ’ØŸ

**Physical AI** (Ø¬Ø³Û’ **Embodied AI** Ø¨Ú¾ÛŒ Ú©ÛØ§ Ø¬Ø§ØªØ§ ÛÛ’) Ø§Ù† Ù…ØµÙ†ÙˆØ¹ÛŒ Ø°ÛØ§Ù†Øª Ú©Û’ Ù†Ø¸Ø§Ù…ÙˆÚº Ú©Ùˆ Ú©ÛØªÛ’ ÛÛŒÚº Ø¬Ùˆ Ø¬Ø³Ù…Ø§Ù†ÛŒ Ø¯Ù†ÛŒØ§ Ú©Ùˆ Ù…Ø­Ø³ÙˆØ³ Ú©Ø±ØªÛ’ ÛÛŒÚº Ø§ÙˆØ± Ø§Ø³ Ú©Û’ Ø³Ø§ØªÚ¾ ØªØ¹Ø§Ù…Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚºÛ” Ø®Ø§Ù„Øµ software AI Ù†Ø¸Ø§Ù…ÙˆÚº Ø¬ÛŒØ³Û’ ChatGPT ÛŒØ§ DALL-E Ú©Û’ Ø¨Ø±Ø¹Ú©Ø³ Ø¬Ùˆ Ù…Ú©Ù…Ù„ Ø·ÙˆØ± Ù¾Ø± ÚˆÛŒØ¬ÛŒÙ¹Ù„ Ø¯Ù†ÛŒØ§ Ù…ÛŒÚº Ú©Ø§Ù… Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ Physical AI Ù†Ø¸Ø§Ù…ÙˆÚº Ú©Ùˆ Ù„Ø§Ø²Ù…ÛŒ Ø·ÙˆØ± Ù¾Ø±:

1. **Ù…Ø­Ø³ÙˆØ³ Ú©Ø±Ù†Ø§**: Ø­Ù‚ÛŒÙ‚ÛŒ Ø¯Ù†ÛŒØ§ Ø³Û’ sensory data Ø¬Ù…Ø¹ Ú©Ø±Ù†Ø§ (cameras, LiDAR, IMUs, touch sensors)
2. **ÙÛŒØµÙ„Û Ú©Ø±Ù†Ø§**: observations Ú©Ùˆ process Ú©Ø±Ù†Ø§ Ø§ÙˆØ± real-time Ù…ÛŒÚº ÙÛŒØµÙ„Û’ Ú©Ø±Ù†Ø§ (<10ms safety-critical control Ú©Û’ Ù„ÛŒÛ’)
3. **Ø¹Ù…Ù„ Ú©Ø±Ù†Ø§**: Ø¬Ø³Ù…Ø§Ù†ÛŒ actuators (motors, grippers, wheels) Ú©Ùˆ control Ú©Ø±Ú©Û’ tasks Ù…Ú©Ù…Ù„ Ú©Ø±Ù†Ø§

**Ù…Ø«Ø§Ù„**: Ú©Ù¾Ú‘Û’ ØªÛÛ Ú©Ø±Ù†Û’ ÙˆØ§Ù„Ø§ humanoid robot Physical AI Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªØ§ ÛÛ’:
- **Ù…Ø­Ø³ÙˆØ³ Ú©Ø±ØªØ§ ÛÛ’**: RGB-D camera Ù…ÛŒØ² Ù¾Ø± shirt Ú©Ø§ Ù¾ØªÛ Ù„Ú¯Ø§ØªØ§ ÛÛ’
- **ÙÛŒØµÙ„Û Ú©Ø±ØªØ§ ÛÛ’**: Vision-Language-Action model grasp Ø§ÙˆØ± fold sequence Ú©ÛŒ Ù…Ù†ØµÙˆØ¨Û Ø¨Ù†Ø¯ÛŒ Ú©Ø±ØªØ§ ÛÛ’
- **Ø¹Ù…Ù„ Ú©Ø±ØªØ§ ÛÛ’**: 7-DOF robot arms shirt Ú©Ùˆ pick, fold, Ø§ÙˆØ± place Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ motions execute Ú©Ø±ØªÛ’ ÛÛŒÚº

**Software AI Ú©Û’ Ø³Ø§ØªÚ¾ ØªÙ‚Ø§Ø¨Ù„**:

| Ø®ØµÙˆØµÛŒØª | Software AI (ChatGPT) | Physical AI (Humanoid Robot) |
|---------|----------------------|------------------------------|
| **Ù…Ø§Ø­ÙˆÙ„** | ÚˆÛŒØ¬ÛŒÙ¹Ù„ text/images | Ø¬Ø³Ù…Ø§Ù†ÛŒ Ø¯Ù†ÛŒØ§ (3D, dynamic, ØºÛŒØ± Ù…ØªÙˆÙ‚Ø¹) |
| **Latency Tolerance** | Ø³ÛŒÚ©Ù†Úˆ OK | <10ms control Ú©Û’ Ù„ÛŒÛ’ (collision avoidance) |
| **Error Cost** | ØºÙ„Ø· Ø¬ÙˆØ§Ø¨ | Ø¬Ø³Ù…Ø§Ù†ÛŒ Ù†Ù‚ØµØ§Ù†ØŒ Ú†ÙˆÙ¹ |
| **Sensors** | Ú©ÙˆØ¦ÛŒ Ù†ÛÛŒÚº (ØµØ±Ù text input) | Cameras, LiDAR, IMU, force sensors |
| **Actuators** | Ú©ÙˆØ¦ÛŒ Ù†ÛÛŒÚº (ØµØ±Ù text output) | Motors, grippers, wheels |
| **Deployment** | Cloud servers | Edge devices (Jetson Orin Nano) |

**Physical AI Ù…Ø´Ú©Ù„ Ú©ÛŒÙˆÚº ÛÛ’**:
- **Real-time constraints**: Software AI 10 Ø³ÛŒÚ©Ù†Úˆ "Ø³ÙˆÚ†" Ø³Ú©ØªØ§ ÛÛ’Ø› robots Ú©Ùˆ collision Ø³Û’ Ø¨Ú†Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ 10 milliseconds Ù…ÛŒÚº react Ú©Ø±Ù†Ø§ ÛÙˆØªØ§ ÛÛ’Û”
- **Sim-to-real gap**: Simulation (Isaac Sim, Gazebo) Ù…ÛŒÚº training Ø­Ù‚ÛŒÙ‚ÛŒ hardware Ù¾Ø± perfectly transfer Ù†ÛÛŒÚº ÛÙˆØªÛŒ physics Ú©Û’ ÙØ±Ù‚ Ú©ÛŒ ÙˆØ¬Û Ø³Û’Û”
- **Safety-critical**: ChatGPT Ù…ÛŒÚº ØºÙ„Ø· Ø¬ÙˆØ§Ø¨ irritating ÛÛ’Ø› robot Ù…ÛŒÚº ØºÙ„Ø· motion Ú†ÙˆÙ¹ Ú©Ø§ Ø³Ø¨Ø¨ Ø¨Ù† Ø³Ú©ØªÛŒ ÛÛ’Û”
- **Embodiment matters**: ÛØ± robot Ú©ÛŒ Ù…Ø®ØªÙ„Ù morphology ÛÛ’ (humanoid vs. quadruped vs. industrial arm), Ø¬Ø³ Ø³Û’ model adaptation Ø¶Ø±ÙˆØ±ÛŒ ÛÙˆØªÛŒ ÛÛ’Û”

---

## Ø§Ø¨ Ú©ÛŒÙˆÚºØŸ Physical AI Ú©Û’ Ù„ÛŒÛ’ Ø¨ÛØªØ±ÛŒÙ† ÙˆÙ‚Øª

ØªÛŒÙ† Ø¨ÛŒÚ© ÙˆÙ‚Øª breakthroughs Ù†Û’ 2025 Ú©Ùˆ Physical AI systems Ø¨Ù†Ø§Ù†Û’ Ú©Ø§ **Ø³Ù†ÛØ±ÛŒ Ø¯ÙˆØ±** Ø¨Ù†Ø§ Ø¯ÛŒØ§:

### 1. Vision-Language-Action (VLA) Models

**Ú©ÛŒØ§ Ø¨Ø¯Ù„Ø§**: 2022-2024 Ù…ÛŒÚºØŒ Google DeepMind, UC Berkeley, Ø§ÙˆØ± Stanford Ù†Û’ open VLA models release Ú©ÛŒÛ’ Ø¬Ùˆ combine Ú©Ø±ØªÛ’ ÛÛŒÚº:
- **Vision**: RGB-D camera images
- **Language**: Natural language instructions ("cup Ø§Ù¹Ú¾Ø§Ø¤")
- **Action**: Direct robot motor commands

**Ù…Ø«Ø§Ù„ÛŒÚº**:
- **RT-1** (Google, 2022): 130,000+ robot demonstrations, 62% success novel tasks Ù¾Ø±
- **RT-2** (Google DeepMind, 2023): Web-scale vision-language knowledge Ú©Ùˆ robotics Ù…ÛŒÚº transfer Ú©Ø±ØªØ§ ÛÛ’
- **Octo** (UC Berkeley, 2024): Open-source generalist policy, Ú©Ù… data Ú©Û’ Ø³Ø§ØªÚ¾ fine-tunable
- **OpenVLA** (Stanford, 2024): 7B parameter model 970k robot trajectories Ù¾Ø± trained

**ÛŒÛ Ú©ÛŒÙˆÚº Ø§ÛÙ… ÛÛ’**: VLA models Ø³Û’ Ù¾ÛÙ„Û’, robots Ú©Ùˆ program Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ manual trajectory planning, inverse kinematics, Ø§ÙˆØ± task-specific controllers Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ØªÚ¾ÛŒÛ” Ø§Ø¨, Ø¢Ù¾ Ø§ÛŒÚ© single model Ú©Ùˆ diverse tasks Ù¾Ø± train Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº Ø§ÙˆØ± Ù†Ø¦Û’ objects/instructions Ú©Û’ Ù„ÛŒÛ’ generalize Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”

### 2. Ø³Ø³ØªØ§ Edge AI Hardware

**Ú©ÛŒØ§ Ø¨Ø¯Ù„Ø§**: NVIDIA Jetson family Ù†Û’ datacenter AI performance Ú©Ùˆ $249 devices Ù…ÛŒÚº Ù„Ø§ Ø¯ÛŒØ§Û”

**Economy Jetson Kit (ÛŒÛ Ú©ÙˆØ±Ø³)**:

| Component | Ù‚ÛŒÙ…Øª | Specs | Ù…Ù‚ØµØ¯ |
|-----------|-------|-------|---------|
| **Jetson Orin Nano** | **$249** | 10 TOPS INT8, 8GB RAM | VLA model inference Ú†Ù„Ø§Ù†Ø§ (<10ms) |
| **RealSense D435i** | **$349** | RGB-D camera, IMU | Perception (30 FPS Ù¾Ø± depth + color) |
| Power + Cables + SD | $100 | 65W USB-C, 128GB storage | Power Ø§ÙˆØ± storage |
| **Ú©Ù„** | **$700** | - | Ù…Ú©Ù…Ù„ edge AI robot brain |

**ÛŒÛ Ú©ÛŒÙˆÚº Ø§ÛÙ… ÛÛ’**: 2020 Ù…ÛŒÚºØŒ Ù…Ø³Ø§ÙˆÛŒ performance Ú©Û’ Ù„ÛŒÛ’ $5,000+ GPUs Ø§ÙˆØ± desktop PCs Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ØªÚ¾ÛŒÛ” 2025 Ù…ÛŒÚºØŒ Ø§ÛŒÚ© $249 Jetson 7-15W power Ú©Û’ Ø³Ø§ØªÚ¾ ÙˆÛÛŒ models Ú†Ù„Ø§ØªØ§ ÛÛ’ (battery-powered robots Ù…Ù…Ú©Ù† ÛÛŒÚº)Û”

**Cloud Alternative**: AWS g5.xlarge ($1.006/hr) = $205/quarter 5 hrs/week Ú©Û’ Ù„ÛŒÛ’Û” Jetson ~3.5 quarters Ú©Û’ Ø¨Ø¹Ø¯ Ø³Ø³ØªØ§ (Ø§ÙˆØ± Ø¢Ù¾ hardware Ú©Û’ Ù…Ø§Ù„Ú© ÛÛŒÚº)Û”

### 3. Mature Sim-to-Real Workflows

**Ú©ÛŒØ§ Ø¨Ø¯Ù„Ø§**: Simulation platforms (Isaac Sim, Gazebo, MuJoCo) Ø§Ø¨ physics, sensors, Ø§ÙˆØ± lighting Ú©Ùˆ accurately model Ú©Ø±ØªÛ’ ÛÛŒÚº effective sim-to-real transfer Ú©Û’ Ù„ÛŒÛ’Û”

**ØµØ­ÛŒØ­ Workflow** (Ø§Ø³ Ú©ÙˆØ±Ø³ Ù…ÛŒÚº ÛØ± Ø¬Ú¯Û enforced):

```
1. Cloud Ù…ÛŒÚº Train Ú©Ø±ÛŒÚº â†’ 2. Model Export Ú©Ø±ÛŒÚº â†’ 3. Jetson Ù¾Ø± Deploy Ú©Ø±ÛŒÚº â†’ 4. Locally Run Ú©Ø±ÛŒÚº
```

**ÛŒÛ Ú©ÛŒÙˆÚº Ø§ÛÙ… ÛÛ’**:
- **Cloud Ù…ÛŒÚº training**: A10G/A100 GPUs Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº parallel Ù…ÛŒÚº 10,000+ episodes Ú©ÛŒ training Ú©Û’ Ù„ÛŒÛ’
- **Edge Ù¾Ø± deploy**: 50-200ms+ network latency Ø³Û’ Ø¨Ú†ÛŒÚº (real-time control Ú©Ùˆ unsafe Ø¨Ù†Ø§ Ø¯ÛŒØªÛŒ ÛÛ’)
- **Isaac Sim**: NVIDIA Ú©Ø§ photorealistic simulator RTX raytracing Ú©Û’ Ø³Ø§ØªÚ¾ accurate RGB/depth Ú©Û’ Ù„ÛŒÛ’
- **Domain randomization**: Sim Ù…ÛŒÚº lighting, textures, object sizes vary Ú©Ø±ÛŒÚº â†’ models real world Ù…ÛŒÚº generalize Ú©Ø±ØªÛ’ ÛÛŒÚº

âš ï¸ **LATENCY TRAP ÙˆØ§Ø±Ù†Ù†Ú¯** âš ï¸

Ú©Ø¨Ú¾ÛŒ Ø¨Ú¾ÛŒ Ø§ØµÙ„ robot Ú©Ùˆ cloud Ø³Û’ directly control Ù†Û Ú©Ø±ÛŒÚºÛ” Network latency (50-200ms+) real-time control Ú©Ùˆ unsafe Ø¨Ù†Ø§ØªÛŒ ÛÛ’Û” ÛÙ…ÛŒØ´Û models Ú©Ùˆ edge devices (Jetson) Ù¾Ø± deploy Ú©Ø±ÛŒÚº <10ms inference Ú©Û’ Ù„ÛŒÛ’Û”

---

## Ú©ÙˆØ±Ø³ Ú©Ø§ Ø¬Ø§Ø¦Ø²Û: 4 Modules â†’ 1 Capstone

ÛŒÛ Ú©ÙˆØ±Ø³ Ø¢Ù¾ Ú©Ùˆ **industry-standard robotics stack** Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Physical AI systems Ø¨Ù†Ø§Ù†Ø§ Ø³Ú©Ú¾Ø§ØªØ§ ÛÛ’:

### Module 1: Robotic Nervous System (ROS 2)
**Ù…Ø¯Øª**: 4 ÛÙØªÛ’ | **Hardware**: Ú©ÙˆØ¦ÛŒ Ø¨Ú¾ÛŒ Linux machine (VM OK)

ROS 2 (Robot Operating System 2) Ø³ÛŒÚ©Ú¾ÛŒÚºØŒ ÙˆÛ middleware Ø¬Ùˆ robot components Ú©Ùˆ Ø¬ÙˆÚ‘ØªØ§ ÛÛ’:
- **ÛÙØªÛ 1**: Nodes, topics (sensor data streaming Ú©Û’ Ù„ÛŒÛ’ publish-subscribe)
- **ÛÙØªÛ 2**: Services (calculations Ú©Û’ Ù„ÛŒÛ’ request-response), actions (feedback Ú©Û’ Ø³Ø§ØªÚ¾ long-running tasks)
- **ÛÙØªÛ 3**: URDF (robot descriptions), TF2 (coordinate transforms)
- **ÛÙØªÛ 4**: Package creation, launch files, parameters

**Ø¢Ø®Ø± Ù…ÛŒÚº**: Ø¢Ù¾ simulated sensors Ø§ÙˆØ± controllers Ú©Û’ Ø³Ø§ØªÚ¾ Ø§ÛŒÚ© multi-node ROS 2 system implement Ú©Ø±ÛŒÚº Ú¯Û’Û”

### Module 2: Simulation Environments (Gazebo & Unity)
**Ù…Ø¯Øª**: 3 ÛÙØªÛ’ | **Hardware**: RTX GPU recommended (cloud alternative: AWS)

Hardware deployment Ø³Û’ Ù¾ÛÙ„Û’ robot policies Ú©ÛŒ training Ú©Û’ Ù„ÛŒÛ’ simulation master Ú©Ø±ÛŒÚº:
- **ÛÙØªÛ 1**: Gazebo setup, URDF/SDF models, physics configuration
- **ÛÙØªÛ 2**: Sensors (cameras, LiDAR, IMU) Ø§ÙˆØ± environment design
- **ÛÙØªÛ 3**: Unity ML-Agents integration (optional: game engine physics Ú©Û’ Ù„ÛŒÛ’)

**Ø¢Ø®Ø± Ù…ÛŒÚº**: Ø¢Ù¾ custom simulation environments Ø¨Ù†Ø§Ø¦ÛŒÚº Ú¯Û’ Ø§ÙˆØ± synthetic training data Ø¬Ù…Ø¹ Ú©Ø±ÛŒÚº Ú¯Û’Û”

### Module 3: Industrial-Grade Simulation (NVIDIA Isaac Sim)
**Ù…Ø¯Øª**: 4 ÛÙØªÛ’ | **Hardware**: RTX GPU Ø¶Ø±ÙˆØ±ÛŒ

NVIDIA Ú©Û’ photorealistic simulator Ú©Û’ Ø³Ø§ØªÚ¾ RTX raytracing Ú©Û’ Ø³Ø§ØªÚ¾ level up Ú©Ø±ÛŒÚº:
- **ÛÙØªÛ 1**: Isaac Sim setup, USD (Universal Scene Description) workflows
- **ÛÙØªÛ 2**: Robots import Ú©Ø±Ù†Ø§ (URDF â†’ USD), sensor configuration
- **ÛÙØªÛ 3**: Synthetic data generation (sim-to-real Ú©Û’ Ù„ÛŒÛ’ domain randomization)
- **ÛÙØªÛ 4**: Sim-to-real transfer, trained policies Ú©Ùˆ Jetson Ù¾Ø± deploy Ú©Ø±Ù†Ø§

**Ø¢Ø®Ø± Ù…ÛŒÚº**: Ø¢Ù¾ Isaac Sim Ù…ÛŒÚº Ø§ÛŒÚ© navigation policy train Ú©Ø±ÛŒÚº Ú¯Û’ Ø§ÙˆØ± real/simulated robot Ù¾Ø± deploy Ú©Ø±ÛŒÚº Ú¯Û’Û”

### Module 4: Vision-Language-Action Models
**Ù…Ø¯Øª**: 4 ÛÙØªÛ’ | **Hardware**: Jetson Orin Nano (Economy Kit)

State-of-the-art VLA models train Ø§ÙˆØ± deploy Ú©Ø±ÛŒÚº:
- **ÛÙØªÛ 1**: RT-1 architecture, demonstrations Ø³Û’ imitation learning
- **ÛÙØªÛ 2**: RT-2 (multimodal transformers), language-conditioned policies
- **ÛÙØªÛ 3**: Octo (open-source), custom tasks Ù¾Ø± fine-tuning
- **ÛÙØªÛ 4**: ONNX export, TensorRT optimization, Jetson deployment

**Ø¢Ø®Ø± Ù…ÛŒÚº**: Ø¢Ù¾ Ø§ÛŒÚ© VLA model fine-tune Ú©Ø±ÛŒÚº Ú¯Û’ Ø§ÙˆØ± robot manipulation tasks Ù¾Ø± <50ms inference Ú©Û’ Ù„ÛŒÛ’ Jetson Ù¾Ø± deploy Ú©Ø±ÛŒÚº Ú¯Û’Û”

---

## Prerequisites: Ø¢Ù¾ Ú©Ùˆ Ú©ÛŒØ§ Ø¬Ø§Ù†Ù†Ø§ Ø¶Ø±ÙˆØ±ÛŒ ÛÛ’

### Ø¶Ø±ÙˆØ±ÛŒ Ø¹Ù„Ù…
- **Programming**: Python 3.10+ (classes, async, decorators Ú©Û’ Ø³Ø§ØªÚ¾ comfortable)
- **Linux**: Basic command line (cd, ls, mkdir, ssh, scp)
- **Math**: High school algebra (vectors, matrices)â€”Ú©ÙˆØ¦ÛŒ calculus Ø¶Ø±ÙˆØ±ÛŒ Ù†ÛÛŒÚº

### Optional Ù„ÛŒÚ©Ù† Ù…Ø¯Ø¯Ú¯Ø§Ø±
- **Robotics**: Coordinate frames, kinematics Ø³Û’ ÙˆØ§Ù‚ÙÛŒØª (ÛÙ… scratch Ø³Û’ Ø³Ú©Ú¾Ø§Ø¦ÛŒÚº Ú¯Û’)
- **Machine Learning**: PyTorch basics (ÛÙ… VLA training step-by-step cover Ú©Ø±ÛŒÚº Ú¯Û’)
- **Computer Vision**: OpenCV experience Ù…Ø¯Ø¯ Ú©Ø±ØªØ§ ÛÛ’ Ù„ÛŒÚ©Ù† Ø¶Ø±ÙˆØ±ÛŒ Ù†ÛÛŒÚº

### System Requirements

**Option 1: Economy Jetson Kit ($700 one-time)** â­ ØªØ¬ÙˆÛŒØ² Ú©Ø±Ø¯Û
- Jetson Orin Nano Developer Kit ($249)
- RealSense D435i camera ($349)
- Power supply + cables + 128GB SD card ($100)
- **Pros**: Hardware ÛÙ…ÛŒØ´Û Ú©Û’ Ù„ÛŒÛ’ Ø§Ù¾Ù†Ø§, <10ms inference, battery-powered Ù…Ù…Ú©Ù†
- **Cons**: Upfront cost, 10 TOPS INT8 ØªÚ© Ù…Ø­Ø¯ÙˆØ¯ (VLA inference Ú©Û’ Ù„ÛŒÛ’ Ù¹Ú¾ÛŒÚ©)

**Option 2: Cloud GPUs ($205/quarter 5 hrs/week Ú©Û’ Ù„ÛŒÛ’)**
- AWS g5.xlarge: NVIDIA A10G (24GB VRAM), $1.006/hr
- Ø§Ø³ØªØ¹Ù…Ø§Ù„: VLA models Ú©ÛŒ training Ú©Û’ Ù„ÛŒÛ’, Isaac Sim (RTX Ø¶Ø±ÙˆØ±ÛŒ)
- **Pros**: Ú©ÙˆØ¦ÛŒ upfront cost Ù†ÛÛŒÚº, training Ú©Û’ Ù„ÛŒÛ’ Ø²ÛŒØ§Ø¯Û VRAM
- **Cons**: Recurring cost, 50-200ms+ latency (ØµØ±Ù simulation, Ø§ØµÙ„ robots Ù†ÛÛŒÚº)

**Option 3: Local RTX Workstation** (Ø§Ú¯Ø± Ø¢Ù¾ Ú©Û’ Ù¾Ø§Ø³ Ù¾ÛÙ„Û’ Ø³Û’ ÛÛ’)
- RTX 3060+ (12GB+ VRAM ØªØ¬ÙˆÛŒØ² Ú©Ø±Ø¯Û)
- Ubuntu 22.04 LTS (native ÛŒØ§ dual-boot)
- **Pros**: Ú©ÙˆØ¦ÛŒ ongoing cost Ù†ÛÛŒÚº, Ù…Ú©Ù…Ù„ control
- **Cons**: Ø§Ú¯Ø± Ù†ÛŒØ§ Ø®Ø±ÛŒØ¯ÛŒÚº ØªÙˆ high upfront cost

---

## Ø¢Ù¾ Ú©Ø§ Ù¾ÛÙ„Ø§ ROS 2 Program (5 Minutes)

Ø¢Ø¦ÛŒÛ’ Ø§Ù¾Ù†ÛŒ ROS 2 installation verify Ú©Ø±ÛŒÚº Ø§ÙˆØ± Ø§ÛŒÚ© minimal node Ú†Ù„Ø§Ø¦ÛŒÚº:

### Step 1: ROS 2 Humble Install Ú©Ø±ÛŒÚº

```bash
# Package list update Ú©Ø±ÛŒÚº
sudo apt update && sudo apt upgrade -y

# ROS 2 repository add Ú©Ø±ÛŒÚº
sudo apt install software-properties-common
sudo add-apt-repository universe
sudo apt install curl -y
curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.asc | sudo gpg --dearmor -o /usr/share/keyrings/ros-archive-keyring.gpg

# ROS 2 Humble Desktop install Ú©Ø±ÛŒÚº
sudo apt update
sudo apt install ros-humble-desktop -y

# ROS 2 environment source Ú©Ø±ÛŒÚº
echo "source /opt/ros/humble/setup.bash" >> ~/.bashrc
source ~/.bashrc
```

### Step 2: Ø§Ù¾Ù†Ø§ Ù¾ÛÙ„Ø§ Node Ø¨Ù†Ø§Ø¦ÛŒÚº Ø§ÙˆØ± Ú†Ù„Ø§Ø¦ÛŒÚº

```python
# hello_physical_ai.py
import rclpy
from rclpy.node import Node

class HelloPhysicalAI(Node):
    def __init__(self):
        super().__init__('hello_physical_ai')
        # ROS 2 Ù†ÙˆÚˆ Ø´Ø±ÙˆØ¹ Ú©Ø±ÛŒÚº
        self.get_logger().info('ğŸ¤– Physical AI Ø³Û’ Ø³Ù„Ø§Ù…!')
        self.get_logger().info('Embodied intelligence systems Ø¨Ù†Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ ØªÛŒØ§Ø±!')

def main():
    rclpy.init()
    node = HelloPhysicalAI()
    rclpy.spin(node)
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

**Ø§Ø³Û’ Ú†Ù„Ø§Ø¦ÛŒÚº**:
```bash
python3 hello_physical_ai.py
```

**Ù…ØªÙˆÙ‚Ø¹ output**:
```
[INFO] [hello_physical_ai]: ğŸ¤– Physical AI Ø³Û’ Ø³Ù„Ø§Ù…!
[INFO] [hello_physical_ai]: Embodied intelligence systems Ø¨Ù†Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ ØªÛŒØ§Ø±!
```

Ù…Ø¨Ø§Ø±Ú© ÛÙˆ! Ø¢Ù¾ Ù†Û’ Ø§Ø¨Ú¾ÛŒ Ø§Ù¾Ù†Ø§ Ù¾ÛÙ„Ø§ ROS 2 node Ú†Ù„Ø§ÛŒØ§Û” ÛŒÛ Ø³Ø§Ø¯Û program demonstrate Ú©Ø±ØªØ§ ÛÛ’:
- **rclpy**: ROS 2 Python client library
- **Node**: ROS 2 systems Ú©Ø§ building block (ÛØ± robot component Ø§ÛŒÚ© node ÛÛ’)
- **Logger**: Debugging Ú©Û’ Ù„ÛŒÛ’ built-in logging

---

## Ø§Ú¯Ù„Û’ Ù‚Ø¯Ù…

Physical AI systems Ø¨Ù†Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ ØªÛŒØ§Ø± ÛÛŒÚº? ÛŒÛØ§Úº Ø¢Ù¾ Ú©Ø§ learning path ÛÛ’:

1. **Module 1 Ø´Ø±ÙˆØ¹ Ú©Ø±ÛŒÚº**: [ROS 2 Fundamentals](/docs/01-ros2/index) (4 ÛÙØªÛ’)
2. **Community Ù…ÛŒÚº Ø´Ø§Ù…Ù„ ÛÙˆÚº**: [GitHub Discussions](https://github.com/your-repo/discussions) Ø³ÙˆØ§Ù„Ø§Øª Ú©Û’ Ù„ÛŒÛ’
3. **Hardware ØªÛŒØ§Ø± Ú©Ø±ÛŒÚº**: Jetson Orin Nano ($249) order Ú©Ø±ÛŒÚº Ø§Ú¯Ø± Modules 3-4 target Ú©Ø± Ø±ÛÛ’ ÛÛŒÚº
4. **Dev Environment Ø³ÛŒÙ¹ Ø§Ù¾ Ú©Ø±ÛŒÚº**: ROS 2 Humble, VS Code, Git install Ú©Ø±ÛŒÚº

**Ù…Ø·Ø§Ù„Ø¹Û’ Ú©ÛŒ ØªØ¬Ø§ÙˆÛŒØ²**:
- ÛÙØªÛ’ Ù…ÛŒÚº 10-15 Ú¯Ú¾Ù†Ù¹Û’ allocate Ú©Ø±ÛŒÚº (2 Ú¯Ú¾Ù†Ù¹Û’ reading + 8 Ú¯Ú¾Ù†Ù¹Û’ hands-on labs + 5 Ú¯Ú¾Ù†Ù¹Û’ projects)
- Chapters Ù¾Ú‘Ú¾Ù†Û’ Ú©Û’ ÙÙˆØ±Ø§Ù‹ Ø¨Ø¹Ø¯ labs Ù…Ú©Ù…Ù„ Ú©Ø±ÛŒÚº (theory Ø¬Ù…Ø¹ Ù†Û ÛÙˆÙ†Û’ Ø¯ÛŒÚº)
- ROS 2 documentation Ø§Ú©Ø«Ø± Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº ([docs.ros.org](https://docs.ros.org/en/humble/))
- Actively debug Ú©Ø±ÛŒÚº (error messages Ù¾Ú‘Ú¾ÛŒÚº, GitHub issues search Ú©Ø±ÛŒÚº)

---

## Resources

- **ROS 2 Humble Docs**: https://docs.ros.org/en/humble/
- **NVIDIA Jetson**: https://developer.nvidia.com/embedded/jetson-orin-nano-developer-kit
- **Isaac Sim**: https://developer.nvidia.com/isaac-sim
- **OpenVLA**: https://github.com/openvla/openvla
- **Octo**: https://github.com/octo-models/octo

---

<div style={{textAlign: 'center', marginTop: '3rem', padding: '2rem', backgroundColor: 'var(--ifm-color-emphasis-100)', borderRadius: '8px'}}>
  <h2>ğŸš€ Physical AI Ù…ÛŒÚº Ø®ÙˆØ´ Ø¢Ù…Ø¯ÛŒØ¯</h2>
  <p style={{fontSize: '1.1rem', marginTop: '1rem'}}>
    Ø¢Ù¾ Ø³Ø¨ Ø³Û’ transformative AI skill Ø³ÛŒÚ©Ú¾Ù†Û’ Ø¬Ø§ Ø±ÛÛ’ ÛÛŒÚº: software Ú©Ùˆ robotic hardware Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ø­Ù‚ÛŒÙ‚ÛŒ Ø¯Ù†ÛŒØ§ Ú©Û’ Ø³Ø§ØªÚ¾ interact Ú©Ø±Ù†Ø§ Ø³Ú©Ú¾Ø§Ù†Ø§Û”
  </p>
  <div style={{marginTop: '2rem'}}>
    <a
      className="button button--primary button--lg"
      href="/docs/01-ros2/index"
    >
      Module 1 Ø´Ø±ÙˆØ¹ Ú©Ø±ÛŒÚº: ROS 2 Fundamentals â†’
    </a>
  </div>
</div>

---

<!-- Generated by @urdu-translator on 2025-12-06 -->
